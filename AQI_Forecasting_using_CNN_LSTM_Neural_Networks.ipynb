{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ybATGdiKRkhH"
      },
      "source": [
        "#V2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "id": "Mbzpu9Y3RoGq",
        "outputId": "0a57dfb3-88d9-4aad-d0b1-18b998d6cc90"
      },
      "outputs": [
        {
          "ename": "KeyboardInterrupt",
          "evalue": "Interrupted by user",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-a6982776cbf4>\u001b[0m in \u001b[0;36m<cell line: 107>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    106\u001b[0m \u001b[0;31m# Prompt the user for a valid city name\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 108\u001b[0;31m     \u001b[0mcity_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Enter the city name: \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    109\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcity_name\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mavailable_cities\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m         \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36mraw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m    849\u001b[0m                 \u001b[0;34m\"raw_input was called, but this frontend does not support input requests.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    850\u001b[0m             )\n\u001b[0;32m--> 851\u001b[0;31m         return self._input_request(str(prompt),\n\u001b[0m\u001b[1;32m    852\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_ident\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    853\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_header\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m    893\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m                 \u001b[0;31m# re-raise KeyboardInterrupt, to truncate traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 895\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Interrupted by user\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    896\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Invalid Message:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_info\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: Interrupted by user"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv1D, MaxPooling1D, Flatten, Reshape, LSTM, Dense\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Load the data\n",
        "data = pd.read_csv('Combined_PTF04_Revised.csv', low_memory=False)\n",
        "\n",
        "# Parse datetime column with errors='coerce' to handle inconsistent formats\n",
        "data['datetime'] = pd.to_datetime(data['datetime'], errors='coerce')\n",
        "\n",
        "# Drop rows with NaT in datetime column\n",
        "data = data.dropna(subset=['datetime'])\n",
        "\n",
        "# Function to preprocess data for a specific city\n",
        "def preprocess_data(city_name, data, sequence_length=14):\n",
        "    # Filter data for the specified city\n",
        "    city_data = data[data['city_name'] == city_name].sort_values('datetime')\n",
        "\n",
        "    # Select relevant columns and drop missing values\n",
        "    city_data = city_data[['datetime', 'main.aqi', 'components.co', 'components.no2', 'components.o3', 'components.so2', 'components.pm2_5', 'components.pm10']].dropna()\n",
        "\n",
        "    # Normalize the data\n",
        "    scaler = MinMaxScaler()\n",
        "    scaled_data = scaler.fit_transform(city_data.iloc[:, 1:])\n",
        "\n",
        "    # Create sequences\n",
        "    X, y = [], []\n",
        "    for i in range(len(scaled_data) - sequence_length):\n",
        "        X.append(scaled_data[i:i+sequence_length])\n",
        "        y.append(scaled_data[i+sequence_length])\n",
        "\n",
        "    X = np.array(X)\n",
        "    y = np.array(y)\n",
        "\n",
        "    return X, y, scaler, city_data\n",
        "\n",
        "# Reshape X for the Conv1D layer (requires 3D input)\n",
        "def reshape_input(X):\n",
        "    return X.reshape((X.shape[0], X.shape[1], X.shape[2]))\n",
        "\n",
        "# Define the model\n",
        "def create_cnn_lstm_model(input_shape):\n",
        "    model = Sequential()\n",
        "    model.add(Conv1D(32, 2, activation='relu', input_shape=input_shape))\n",
        "    model.add(MaxPooling1D(2))\n",
        "    model.add(Flatten())\n",
        "    model.add(Reshape((-1, 32)))\n",
        "    model.add(LSTM(50, activation='relu'))\n",
        "    model.add(Dense(7))\n",
        "    model.compile(optimizer='adam', loss='mse')\n",
        "    return model\n",
        "\n",
        "# Function to make predictions for today based on the latest sequence\n",
        "def predict_future_aqi(city_name, data, model, scaler, sequence_length=14, future_days=7):\n",
        "    X, _, _, city_data = preprocess_data(city_name, data, sequence_length)\n",
        "    X_reshaped = reshape_input(X)\n",
        "\n",
        "    # Get the latest sequence to make future predictions\n",
        "    latest_sequence = X_reshaped[-1].reshape(1, sequence_length, -1)\n",
        "\n",
        "    predictions = []\n",
        "    for _ in range(future_days):\n",
        "        prediction = model.predict(latest_sequence)\n",
        "        predictions.append(prediction[0])\n",
        "\n",
        "        # Append the prediction to the latest_sequence and remove the oldest entry\n",
        "        latest_sequence = np.append(latest_sequence[:, 1:, :], prediction.reshape(1, 1, -1), axis=1)\n",
        "\n",
        "    predictions = scaler.inverse_transform(predictions)\n",
        "\n",
        "    return predictions\n",
        "\n",
        "\n",
        "# Function to plot today's prediction\n",
        "def plot_future_predictions(predictions, city_name):\n",
        "    pollutants = ['main.aqi', 'components.co', 'components.no2', 'components.o3', 'components.so2', 'components.pm2_5', 'components.pm10']\n",
        "    future_days = len(predictions)\n",
        "\n",
        "    fig, axes = plt.subplots(len(pollutants), 1, figsize=(10, 18))\n",
        "    fig.suptitle(f\"Future AQI Predictions for {city_name}\", fontsize=16)\n",
        "\n",
        "    for i, pollutant in enumerate(pollutants):\n",
        "        axes[i].plot(range(future_days), predictions[:, i], marker='o', linestyle='-')\n",
        "        axes[i].set_ylabel(pollutant)\n",
        "        axes[i].set_xlim(0, future_days-1)\n",
        "        axes[i].set_xticks(range(future_days))\n",
        "        axes[i].set_xticklabels([f'Day {j+1}' for j in range(future_days)])\n",
        "\n",
        "        # Annotate the points with the predicted values\n",
        "        for j in range(future_days):\n",
        "            axes[i].annotate(f'{predictions[j, i]:.2f}', (j, predictions[j, i]), textcoords=\"offset points\", xytext=(0,10), ha='center')\n",
        "\n",
        "    plt.tight_layout(rect=[0, 0, 1, 0.96])  # Adjust layout to make room for the title\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Get the list of available cities in the dataset\n",
        "available_cities = data['city_name'].unique()\n",
        "\n",
        "# Prompt the user for a valid city name\n",
        "while True:\n",
        "    city_name = input(\"Enter the city name: \")\n",
        "    if city_name in available_cities:\n",
        "        break\n",
        "    else:\n",
        "        print(f\"City '{city_name}' not found in the dataset. Please try again.\")\n",
        "\n",
        "# Preprocess data for the specified city and create the model\n",
        "X, y, scaler, _ = preprocess_data(city_name, data)\n",
        "X_reshaped = reshape_input(X)\n",
        "input_shape = (X_reshaped.shape[1], X_reshaped.shape[2])\n",
        "model = create_cnn_lstm_model(input_shape)\n",
        "\n",
        "# Train the model on all data\n",
        "model.fit(X_reshaped, y, epochs=10, batch_size=32, validation_split=0.2)\n",
        "\n",
        "# Predict the future AQI for the next 7 days\n",
        "future_predictions = predict_future_aqi(city_name, data, model, scaler, sequence_length=14, future_days=7)\n",
        "\n",
        "# Plot the future predictions\n",
        "plot_future_predictions(future_predictions, city_name)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xTwwSV-oZQOQ"
      },
      "source": [
        "#Anvil"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "XnFh2j6dUJA5",
        "outputId": "e55c13d4-dc0e-47fa-bca5-b1ad48e212c1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting anvil-uplink\n",
            "  Downloading anvil_uplink-0.5.0-py2.py3-none-any.whl (95 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m95.8/95.8 kB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (2.0.3)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (1.2.2)\n",
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.10/dist-packages (2.15.0)\n",
            "Collecting argparse (from anvil-uplink)\n",
            "  Downloading argparse-1.4.0-py2.py3-none-any.whl (23 kB)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.10/dist-packages (from anvil-uplink) (0.18.3)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from anvil-uplink) (1.16.0)\n",
            "Collecting ws4py (from anvil-uplink)\n",
            "  Downloading ws4py-0.5.1.tar.gz (51 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m51.4/51.4 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.1)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from pandas) (1.25.2)\n",
            "Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.11.4)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (3.5.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=23.5.26 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (24.3.25)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.5.4)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.9.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (18.1.1)\n",
            "Requirement already satisfied: ml-dtypes~=0.2.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.3.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow) (24.0)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.20.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow) (67.7.2)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (4.12.1)\n",
            "Requirement already satisfied: wrapt<1.15,>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.14.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.37.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.64.1)\n",
            "Requirement already satisfied: tensorboard<2.16,>=2.15 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.15.2)\n",
            "Requirement already satisfied: tensorflow-estimator<2.16,>=2.15.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.15.0)\n",
            "Requirement already satisfied: keras<2.16,>=2.15.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.15.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow) (0.43.0)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (2.27.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<2,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (1.2.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (3.6)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (2.31.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (3.0.3)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (5.3.3)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (0.4.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow) (1.3.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow) (2024.6.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.16,>=2.15->tensorflow) (2.1.5)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (0.6.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow) (3.2.2)\n",
            "Building wheels for collected packages: ws4py\n",
            "  Building wheel for ws4py (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for ws4py: filename=ws4py-0.5.1-py3-none-any.whl size=45228 sha256=5dac98a39e6b41d619d09a9de5c59f8264a61f85bce52279650afb83f1e2baeb\n",
            "  Stored in directory: /root/.cache/pip/wheels/2e/7c/ad/d9c746276bf024d44296340869fcb169f1e5d80fb147351a57\n",
            "Successfully built ws4py\n",
            "Installing collected packages: ws4py, argparse, anvil-uplink\n",
            "Successfully installed anvil-uplink-0.5.0 argparse-1.4.0 ws4py-0.5.1\n"
          ]
        },
        {
          "data": {
            "application/vnd.colab-display-data+json": {
              "id": "0743e3c0440d49f79eb296587b215ade",
              "pip_warning": {
                "packages": [
                  "argparse",
                  "google"
                ]
              }
            }
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Connecting to wss://anvil.works/uplink\n",
            "Anvil websocket open\n",
            "Connected to \"Default Environment\" as SERVER\n",
            "Epoch 1/10\n",
            "39/39 [==============================] - 3s 21ms/step - loss: 0.0926 - val_loss: 0.0368\n",
            "Epoch 2/10\n",
            "39/39 [==============================] - 0s 11ms/step - loss: 0.0166 - val_loss: 0.0286\n",
            "Epoch 3/10\n",
            "39/39 [==============================] - 0s 10ms/step - loss: 0.0149 - val_loss: 0.0263\n",
            "Epoch 4/10\n",
            "39/39 [==============================] - 0s 10ms/step - loss: 0.0141 - val_loss: 0.0233\n",
            "Epoch 5/10\n",
            "39/39 [==============================] - 0s 10ms/step - loss: 0.0136 - val_loss: 0.0231\n",
            "Epoch 6/10\n",
            "39/39 [==============================] - 0s 11ms/step - loss: 0.0129 - val_loss: 0.0227\n",
            "Epoch 7/10\n",
            "39/39 [==============================] - 0s 11ms/step - loss: 0.0129 - val_loss: 0.0218\n",
            "Epoch 8/10\n",
            "39/39 [==============================] - 1s 13ms/step - loss: 0.0126 - val_loss: 0.0237\n",
            "Epoch 9/10\n",
            "39/39 [==============================] - 0s 8ms/step - loss: 0.0127 - val_loss: 0.0232\n",
            "Epoch 10/10\n",
            "39/39 [==============================] - 0s 6ms/step - loss: 0.0123 - val_loss: 0.0221\n",
            "1/1 [==============================] - 0s 236ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n"
          ]
        }
      ],
      "source": [
        "!pip install anvil-uplink pandas scikit-learn tensorflow\n",
        "\n",
        "import anvil.server\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv1D, MaxPooling1D, Flatten, Reshape, LSTM, Dense\n",
        "\n",
        "# Connect to Anvil\n",
        "anvil.server.connect(\"server_DYNDZNG3KBNFJNT3EUQTKTQ6-PLY2MVWZBUVGYSMJ\")\n",
        "\n",
        "# Load the data directly from the Colab environment\n",
        "data = pd.read_csv('Combined_PTF04_Revised.csv', low_memory=False)\n",
        "data['datetime'] = pd.to_datetime(data['datetime'], errors='coerce')\n",
        "data = data.dropna(subset=['datetime'])\n",
        "\n",
        "# Extract unique city names\n",
        "unique_city_names = list(data['city_name'].unique())\n",
        "\n",
        "scaler = None\n",
        "model = None\n",
        "\n",
        "def preprocess_data(city_name, data, sequence_length=14):\n",
        "    city_data = data[data['city_name'] == city_name].sort_values('datetime')\n",
        "    city_data = city_data[['datetime', 'main.aqi', 'components.co', 'components.no2', 'components.o3', 'components.so2', 'components.pm2_5', 'components.pm10']].dropna()\n",
        "    scaler = MinMaxScaler()\n",
        "    scaled_data = scaler.fit_transform(city_data.iloc[:, 1:])\n",
        "    X, y = [], []\n",
        "    for i in range(len(scaled_data) - sequence_length):\n",
        "        X.append(scaled_data[i:i+sequence_length])\n",
        "        y.append(scaled_data[i+sequence_length])\n",
        "    X = np.array(X)\n",
        "    y = np.array(y)\n",
        "    return X, y, scaler, city_data\n",
        "\n",
        "def reshape_input(X):\n",
        "    return X.reshape((X.shape[0], X.shape[1], X.shape[2]))\n",
        "\n",
        "def create_cnn_lstm_model(input_shape):\n",
        "    model = Sequential()\n",
        "    model.add(Conv1D(32, 2, activation='relu', input_shape=input_shape))\n",
        "    model.add(MaxPooling1D(2))\n",
        "    model.add(Flatten())\n",
        "    model.add(Reshape((-1, 32)))\n",
        "    model.add(LSTM(50, activation='relu'))\n",
        "    model.add(Dense(7))\n",
        "    model.compile(optimizer='adam', loss='mse')\n",
        "    return model\n",
        "\n",
        "@anvil.server.callable\n",
        "def train_model(city_name):\n",
        "    global model, scaler\n",
        "    X, y, scaler, _ = preprocess_data(city_name, data)\n",
        "    X_reshaped = reshape_input(X)\n",
        "    input_shape = (X_reshaped.shape[1], X_reshaped.shape[2])\n",
        "    model = create_cnn_lstm_model(input_shape)\n",
        "    model.fit(X_reshaped, y, epochs=10, batch_size=32, validation_split=0.2)\n",
        "    return {\"status\": \"Model trained successfully for city: \" + city_name}\n",
        "\n",
        "@anvil.server.callable\n",
        "def get_predictions(city_name):\n",
        "    global model, scaler\n",
        "    sequence_length = 14\n",
        "    future_days = 7\n",
        "    X, _, _, city_data = preprocess_data(city_name, data, sequence_length)\n",
        "    X_reshaped = reshape_input(X)\n",
        "    latest_sequence = X_reshaped[-1].reshape(1, sequence_length, -1)\n",
        "    predictions = []\n",
        "    for _ in range(future_days):\n",
        "        prediction = model.predict(latest_sequence)\n",
        "        predictions.append(prediction[0])\n",
        "        latest_sequence = np.append(latest_sequence[:, 1:, :], prediction.reshape(1, 1, -1), axis=1)\n",
        "    predictions = scaler.inverse_transform(predictions)\n",
        "    today_prediction = predictions[0]\n",
        "    future_predictions = predictions\n",
        "    return {\n",
        "        \"today_prediction\": today_prediction.tolist(),\n",
        "        \"future_predictions\": future_predictions.tolist()\n",
        "    }\n",
        "\n",
        "# Keep the server running to accept Anvil calls\n",
        "anvil.server.wait_forever()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8TmP2wuAdDZx"
      },
      "outputs": [],
      "source": [
        "!pip install anvil-uplink pandas scikit-learn tensorflow\n",
        "\n",
        "import anvil.server\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv1D, MaxPooling1D, Flatten, Reshape, LSTM, Dense\n",
        "\n",
        "# Connect to Anvil\n",
        "anvil.server.connect(\"\")\n",
        "\n",
        "# Load the data directly from the Colab environment\n",
        "data = pd.read_csv('Combined_PTF04_Revised.csv', low_memory=False)\n",
        "data['datetime'] = pd.to_datetime(data['datetime'], errors='coerce')\n",
        "data = data.dropna(subset=['datetime'])\n",
        "\n",
        "# Extract unique city names\n",
        "unique_city_names = list(data['city_name'].unique())\n",
        "\n",
        "scaler = None\n",
        "model = None\n",
        "\n",
        "def preprocess_data(city_name, data, sequence_length=14):\n",
        "    city_data = data[data['city_name'] == city_name].sort_values('datetime')\n",
        "    city_data = city_data[['datetime', 'main.aqi', 'components.co', 'components.no2', 'components.o3', 'components.so2', 'components.pm2_5', 'components.pm10']].dropna()\n",
        "    scaler = MinMaxScaler()\n",
        "    scaled_data = scaler.fit_transform(city_data.iloc[:, 1:])\n",
        "    X, y = [], []\n",
        "    for i in range(len(scaled_data) - sequence_length):\n",
        "        X.append(scaled_data[i:i+sequence_length])\n",
        "        y.append(scaled_data[i+sequence_length])\n",
        "    X = np.array(X)\n",
        "    y = np.array(y)\n",
        "    return X, y, scaler, city_data\n",
        "\n",
        "def reshape_input(X):\n",
        "    return X.reshape((X.shape[0], X.shape[1], X.shape[2]))\n",
        "\n",
        "def create_cnn_lstm_model(input_shape):\n",
        "    model = Sequential()\n",
        "    model.add(Conv1D(32, 2, activation='relu', input_shape=input_shape))\n",
        "    model.add(MaxPooling1D(2))\n",
        "    model.add(Flatten())\n",
        "    model.add(Reshape((-1, 32)))\n",
        "    model.add(LSTM(50, activation='relu'))\n",
        "    model.add(Dense(7))\n",
        "    model.compile(optimizer='adam', loss='mse')\n",
        "    return model\n",
        "\n",
        "@anvil.server.callable\n",
        "def train_model(city_name):\n",
        "    global model, scaler\n",
        "    X, y, scaler, _ = preprocess_data(city_name, data)\n",
        "    X_reshaped = reshape_input(X)\n",
        "    input_shape = (X_reshaped.shape[1], X_reshaped.shape[2])\n",
        "    model = create_cnn_lstm_model(input_shape)\n",
        "    model.fit(X_reshaped, y, epochs=10, batch_size=32, validation_split=0.2)\n",
        "    return {\"status\": \"Model trained successfully for city: \" + city_name}\n",
        "\n",
        "@anvil.server.callable\n",
        "def get_predictions(city_name):\n",
        "    global model, scaler\n",
        "    sequence_length = 14\n",
        "    future_days = 7\n",
        "    X, _, _, city_data = preprocess_data(city_name, data, sequence_length)\n",
        "    X_reshaped = reshape_input(X)\n",
        "    latest_sequence = X_reshaped[-1].reshape(1, sequence_length, -1)\n",
        "    predictions = []\n",
        "    for _ in range(future_days):\n",
        "        prediction = model.predict(latest_sequence)\n",
        "        predictions.append(prediction[0])\n",
        "        latest_sequence = np.append(latest_sequence[:, 1:, :], prediction.reshape(1, 1, -1), axis=1)\n",
        "    predictions = scaler.inverse_transform(predictions)\n",
        "    today_prediction = predictions[0]\n",
        "    future_predictions = predictions\n",
        "    return {\n",
        "        \"today_prediction\": today_prediction.tolist(),\n",
        "        \"future_predictions\": future_predictions.tolist()\n",
        "    }\n",
        "\n",
        "# Keep the server running to accept Anvil calls\n",
        "anvil.server.wait_forever()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#V3\n",
        "\n"
      ],
      "metadata": {
        "id": "s0XUvA4PGQF5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pandas\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UI7SaMYNGTwd",
        "outputId": "3874a81a-8a0f-47cd-f71a-33ccd828fdfa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (2.1.4)\n",
            "Requirement already satisfied: numpy<2,>=1.22.4 in /usr/local/lib/python3.10/dist-packages (from pandas) (1.26.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.1)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load the Excel file\n",
        "file_path = '/content/DATA_04092024.xlsx'  # Colab typically stores files in /content/\n",
        "xls = pd.ExcelFile(file_path)\n",
        "\n",
        "# Check sheet names\n",
        "sheet_names = xls.sheet_names\n",
        "print(\"Sheet Names:\", sheet_names)\n",
        "\n",
        "# Load a specific sheet into a DataFrame\n",
        "df = pd.read_excel(xls, sheet_name=sheet_names[0])  # Replace 0 with the sheet index or name\n",
        "print(df.head())  # Display the first few rows of the data\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NnVXg0xOHIKq",
        "outputId": "bc9c1e0a-217e-4a4e-80e6-152747487101"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sheet Names: ['Sheet1']\n",
            "                    datetime  main.aqi  components.co  components.no2  \\\n",
            "0  2023-11-06 15:52:19+08:00         2         337.12            1.95   \n",
            "1  2023-11-06 15:52:20+08:00         2         343.80            5.01   \n",
            "2  2023-11-06 15:52:22+08:00         1         240.33            1.41   \n",
            "3  2023-11-06 15:52:23+08:00         1         181.91            0.41   \n",
            "4  2023-11-06 15:52:25+08:00         5        1001.36            5.44   \n",
            "\n",
            "   components.o3  components.so2  components.pm2_5  components.pm10  \\\n",
            "0          76.53            1.10             18.51            20.02   \n",
            "1          60.80            3.61              8.12             9.62   \n",
            "2          51.50            0.41              1.28             1.65   \n",
            "3          37.55            1.33              1.12             1.28   \n",
            "4         254.63           28.85            102.82           131.67   \n",
            "\n",
            "      city_name  \n",
            "0      Alaminos  \n",
            "1  Angeles City  \n",
            "2      Antipolo  \n",
            "3       Bacolod  \n",
            "4        Bacoor  \n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}